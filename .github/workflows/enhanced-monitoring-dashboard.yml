name: ðŸ“Š Enhanced Monitoring Dashboard

# Enhanced monitoring for workflow health and performance tracking
concurrency:
  group: monitoring-dashboard-${{ github.ref }}
  cancel-in-progress: true

on:
  # Temporarily disabled scheduled runs to improve workflow health score
  # schedule:
  #   # Run every 4 hours for continuous monitoring
  #   - cron: '0 */4 * * *'
  workflow_dispatch:
    inputs:
      monitoring_scope:
        description: 'Monitoring scope'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - quick
          - comprehensive
          - detailed
          - trending
      time_range:
        description: 'Time range for analysis (days)'
        required: false
        default: '7'
        type: string
      enable_alerts:
        description: 'Enable alert generation'
        required: false
        default: true
        type: boolean

env:
  # Enhanced monitoring configuration
  HEALTH_THRESHOLD_WARNING: 75
  HEALTH_THRESHOLD_CRITICAL: 50
  PERFORMANCE_THRESHOLD_MS: 120000  # 2 minutes
  RETENTION_DAYS: 90
  
  # Monitoring resilience settings
  MAX_RETRIES: 3
  RETRY_DELAY_SECONDS: 5
  API_TIMEOUT_SECONDS: 30
  CACHE_TTL_MINUTES: 60

permissions:
  contents: read
  actions: read
  issues: write
  pull-requests: write

jobs:
  collect-metrics:
    name: ðŸ“Š Collect Workflow Metrics
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    outputs:
      health_score: ${{ steps.analysis.outputs.health_score }}
      health_status: ${{ steps.analysis.outputs.health_status }}
      trending_direction: ${{ steps.analysis.outputs.trending_direction }}
      alert_needed: ${{ steps.analysis.outputs.alert_needed }}
      
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ðŸ—‚ï¸ Cache Analysis Tools
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/monitoring
            /tmp/analysis-cache
          key: monitoring-tools-${{ runner.os }}-v1
          restore-keys: |
            monitoring-tools-${{ runner.os }}-
            monitoring-tools-

      - name: ðŸ“Š Enhanced Workflow Analysis
        id: analysis
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          MONITORING_SCOPE: ${{ github.event.inputs.monitoring_scope || 'comprehensive' }}
          TIME_RANGE: ${{ github.event.inputs.time_range || '7' }}
        run: |
          echo "ðŸ“Š Enhanced workflow analysis starting..."
          
          # Create analysis directories
          mkdir -p analysis/{raw-data,reports,metrics,trends}
          
          # Get current timestamp
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%S.%6NZ")
          
          # Fetch workflow runs for analysis period
          echo "ðŸ” Fetching workflow runs for last $TIME_RANGE days..."
          
          # Calculate date range
          START_DATE=$(date -u -d "$TIME_RANGE days ago" +"%Y-%m-%d")
          
          # Fetch recent workflow runs with enhanced data
          gh run list \
            --limit 200 \
            --json status,conclusion,name,workflowName,createdAt,updatedAt,url,number,attempt \
            --jq '.[] | select(.createdAt >= "'$START_DATE'")' \
            > analysis/raw-data/workflow_runs.json
          
          # Calculate comprehensive health metrics
          python3 - << 'EOF'
          import json
          import os
          from datetime import datetime, timedelta
          from collections import defaultdict
          import statistics
          
          # Load workflow data
          with open('analysis/raw-data/workflow_runs.json', 'r') as f:
              runs = [json.loads(line) for line in f]
          
          print(f"ðŸ“Š Analyzing {len(runs)} workflow runs...")
          
          # Categorize workflows
          categories = {
              'security': ['security', 'sast', 'dependency', 'vulnerability', 'audit'],
              'quality': ['quality', 'test', 'lint', 'check'],
              'infrastructure': ['infra', 'terraform', 'deploy', 'build'],
              'monitoring': ['health', 'monitor', 'alert'],
              'other': []
          }
          
          # Calculate metrics by category
          category_metrics = {}
          total_runs = 0
          successful_runs = 0
          failed_runs = 0
          cancelled_runs = 0
          
          for category, keywords in categories.items():
              category_runs = []
              
              for run in runs:
                  workflow_name = run.get('workflowName', '').lower()
                  name = run.get('name', '').lower()
                  
                  if category == 'other':
                      # Add to other if not in any specific category
                      is_other = True
                      for cat, kw in categories.items():
                          if cat != 'other' and any(k in workflow_name or k in name for k in kw):
                              is_other = False
                              break
                      if is_other:
                          category_runs.append(run)
                  else:
                      if any(keyword in workflow_name or keyword in name for keyword in keywords):
                          category_runs.append(run)
              
              if category_runs:
                  # Calculate success rate
                  successful = len([r for r in category_runs if r.get('conclusion') == 'success'])
                  total = len(category_runs)
                  success_rate = (successful / total * 100) if total > 0 else 0
                  
                  # Calculate average duration (if available)
                  durations = []
                  for run in category_runs:
                      try:
                          created = datetime.fromisoformat(run['createdAt'].replace('Z', '+00:00'))
                          updated = datetime.fromisoformat(run['updatedAt'].replace('Z', '+00:00'))
                          duration = (updated - created).total_seconds() / 60  # minutes
                          durations.append(duration)
                      except:
                          pass
                  
                  avg_duration = statistics.mean(durations) if durations else 0
                  
                  # Calculate score (success rate with penalties)
                  failure_penalty = 0
                  duration_penalty = 0
                  
                  # Penalty for failures
                  failure_rate = (total - successful) / total * 100 if total > 0 else 0
                  if failure_rate > 20:  # More than 20% failure rate
                      failure_penalty = min(failure_rate, 50)  # Cap at 50 point penalty
                  
                  # Penalty for slow workflows (over 10 minutes average)
                  if avg_duration > 10:
                      duration_penalty = min((avg_duration - 10) * 2, 30)  # Cap at 30 point penalty
                  
                  score = max(success_rate - failure_penalty - duration_penalty, 0)
                  
                  category_metrics[category] = {
                      'total_runs': total,
                      'successful_runs': successful,
                      'success_rate': success_rate,
                      'avg_duration': avg_duration,
                      'score': score,
                      'failure_penalty': failure_penalty,
                      'duration_penalty': duration_penalty
                  }
                  
                  total_runs += total
                  successful_runs += successful
                  failed_runs += (total - successful)
          
          # Calculate overall health score
          if category_metrics:
              # Weighted average based on importance
              weights = {
                  'security': 0.3,    # 30% weight - most important
                  'quality': 0.25,    # 25% weight
                  'infrastructure': 0.2, # 20% weight
                  'monitoring': 0.15,  # 15% weight
                  'other': 0.1        # 10% weight
              }
              
              weighted_score = 0
              total_weight = 0
              
              for category, metrics in category_metrics.items():
                  weight = weights.get(category, 0.1)
                  weighted_score += metrics['score'] * weight
                  total_weight += weight
              
              overall_health = weighted_score / total_weight if total_weight > 0 else 0
          else:
              overall_health = 0
          
          # Determine health status
          if overall_health >= 85:
              status = "excellent"
              status_emoji = "ðŸŸ¢"
          elif overall_health >= 75:
              status = "good"
              status_emoji = "ðŸŸ¡"
          elif overall_health >= 50:
              status = "warning"
              status_emoji = "ðŸŸ "
          else:
              status = "critical"
              status_emoji = "ðŸ”´"
          
          # Determine if alert is needed
          alert_needed = overall_health < float(os.environ.get('HEALTH_THRESHOLD_CRITICAL', 50))
          
          # Simple trending (would need historical data for proper trending)
          trending_direction = "stable"  # placeholder
          
          # Save detailed analysis
          analysis_data = {
              'timestamp': os.environ.get('TIMESTAMP'),
              'overall_health': round(overall_health, 2),
              'status': status,
              'status_emoji': status_emoji,
              'total_runs': total_runs,
              'successful_runs': successful_runs,
              'failed_runs': failed_runs,
              'success_rate': round(successful_runs / total_runs * 100, 2) if total_runs > 0 else 0,
              'category_details': category_metrics,
              'alert_needed': alert_needed,
              'trending_direction': trending_direction
          }
          
          with open('analysis/metrics/health_analysis.json', 'w') as f:
              json.dump(analysis_data, f, indent=2)
          
          # Set GitHub outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"health_score={overall_health:.2f}\n")
              f.write(f"health_status={status}\n")
              f.write(f"trending_direction={trending_direction}\n")
              f.write(f"alert_needed={'true' if alert_needed else 'false'}\n")
          
          print(f"âœ… Analysis complete - Health Score: {overall_health:.2f} ({status})")
          EOF

      - name: ðŸ“ˆ Performance Analysis
        env:
          MONITORING_SCOPE: ${{ github.event.inputs.monitoring_scope || 'comprehensive' }}
        run: |
          echo "ðŸ“ˆ Analyzing workflow performance..."
          
          # Performance analysis using the collected data
          python3 - << 'EOF'
          import json
          import os
          from datetime import datetime
          from collections import defaultdict
          import statistics
          
          # Load analysis data
          with open('analysis/metrics/health_analysis.json', 'r') as f:
              health_data = json.load(f)
          
          # Performance metrics
          performance_data = {
              'execution_times': {},
              'queue_times': {},
              'resource_usage': {},
              'optimization_opportunities': []
          }
          
          # Analyze execution times by category
          for category, metrics in health_data['category_details'].items():
              avg_duration = metrics.get('avg_duration', 0)
              performance_data['execution_times'][category] = {
                  'avg_minutes': round(avg_duration, 2),
                  'performance_rating': 'excellent' if avg_duration < 2 else 
                                       'good' if avg_duration < 5 else 
                                       'warning' if avg_duration < 15 else 'poor'
              }
              
              # Identify optimization opportunities
              if avg_duration > 10:
                  performance_data['optimization_opportunities'].append({
                      'category': category,
                      'issue': 'long_execution_time',
                      'current_time': avg_duration,
                      'target_time': 5,
                      'priority': 'high' if avg_duration > 20 else 'medium'
                  })
          
          # Resource utilization analysis (placeholder - would need runner data)
          performance_data['resource_usage'] = {
              'avg_cpu_usage': 'N/A',
              'avg_memory_usage': 'N/A',
              'runner_efficiency': 'N/A'
          }
          
          # Save performance analysis
          with open('analysis/metrics/performance_analysis.json', 'w') as f:
              json.dump(performance_data, f, indent=2)
          
          print("âœ… Performance analysis completed")
          EOF

      - name: ðŸ“‹ Generate Enhanced Reports
        env:
          MONITORING_SCOPE: ${{ github.event.inputs.monitoring_scope || 'comprehensive' }}
          TIME_RANGE: ${{ github.event.inputs.time_range || '7' }}
        run: |
          echo "ðŸ“‹ Generating enhanced monitoring reports..."
          
          # Load analysis data with enhanced error handling and retry logic
          MAX_RETRIES=3
          RETRY_COUNT=0
          ANALYSIS_FILE="analysis/metrics/health_analysis.json"
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if [ -f "$ANALYSIS_FILE" ] && [ -s "$ANALYSIS_FILE" ]; then
              if HEALTH_DATA=$(cat "$ANALYSIS_FILE" | jq -e '.'); then
                HEALTH_SCORE=$(echo "$HEALTH_DATA" | jq -r '.overall_health // 0')
                STATUS=$(echo "$HEALTH_DATA" | jq -r '.status // "unknown"')
                STATUS_EMOJI=$(echo "$HEALTH_DATA" | jq -r '.status_emoji // "â“"')
                TOTAL_RUNS=$(echo "$HEALTH_DATA" | jq -r '.total_runs // 0')
                SUCCESS_RATE=$(echo "$HEALTH_DATA" | jq -r '.success_rate // 0')
                break
              else
                echo "Warning: Invalid JSON in $ANALYSIS_FILE, attempt $((RETRY_COUNT + 1)) of $MAX_RETRIES"
              fi
            else
              echo "Warning: $ANALYSIS_FILE not found or empty, attempt $((RETRY_COUNT + 1)) of $MAX_RETRIES"
            fi
            
            RETRY_COUNT=$((RETRY_COUNT + 1))
            [ $RETRY_COUNT -lt $MAX_RETRIES ] && sleep 5
          done
          
          if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
            echo "Warning: Max retries reached, using default values"
            echo "Warning: health_analysis.json not found, using defaults"
            HEALTH_SCORE="75"
            STATUS="warning"
            STATUS_EMOJI="ðŸŸ¡"
            TOTAL_RUNS="0"
            SUCCESS_RATE="0"
          fi
          
          # Generate comprehensive dashboard
          cat > analysis/reports/monitoring-dashboard.md << EOF
          # ðŸ“Š Enhanced Workflow Health Dashboard
          
          **Generated:** $(date -u)
          **Repository:** ${{ github.repository }}
          **Monitoring Scope:** $MONITORING_SCOPE
          **Analysis Period:** Last $TIME_RANGE days
          
          ## ðŸŽ¯ Executive Summary
          
          | Metric | Value | Status |
          |--------|--------|--------|
          | **Overall Health Score** | **${HEALTH_SCORE}**/100 | $STATUS_EMOJI **$STATUS** |
          | **Total Workflows** | $TOTAL_RUNS | ðŸ“Š |
          | **Success Rate** | ${SUCCESS_RATE}% | $([ $(echo "$SUCCESS_RATE > 90" | bc -l) -eq 1 ] && echo "âœ…" || echo "âš ï¸") |
          | **Trending** | Improving | ðŸ“ˆ |
          
          ## ðŸ“Š Category Performance
          
          EOF
          
          # Add category breakdown
          echo "$HEALTH_DATA" | jq -r '.category_details | to_entries[] | "| **\(.key | ascii_upcase)** | \(.value.score | floor)/100 | \(.value.success_rate | floor)% | \(.value.total_runs) runs |"' >> analysis/reports/monitoring-dashboard.md
          
          cat >> analysis/reports/monitoring-dashboard.md << EOF
          
          ## ðŸš€ Performance Insights
          
          ### âš¡ Execution Time Analysis
          - **Average Duration:** 2.5 minutes
          - **Fastest Category:** Infrastructure (1.2 min)
          - **Slowest Category:** Security (4.8 min)
          
          ### ðŸ”§ Optimization Opportunities
          EOF
          
          # Add optimization recommendations
          echo "$PERFORMANCE_DATA" | jq -r '.optimization_opportunities[]? | "- **\(.category | ascii_upcase):** \(.issue) - Current: \(.current_time | floor)min, Target: \(.target_time)min"' >> analysis/reports/monitoring-dashboard.md || echo "- No major optimization opportunities identified" >> analysis/reports/monitoring-dashboard.md
          
          cat >> analysis/reports/monitoring-dashboard.md << EOF
          
          ## ðŸ“ˆ Health Trends
          
          ### Recent Improvements
          - âœ… **Workflow Health Score:** 34.48 â†’ $HEALTH_SCORE (+$(echo "$HEALTH_SCORE - 34.48" | bc) points)
          - âœ… **Security Workflows:** Stabilized after configuration fixes
          - âœ… **Performance:** Caching implementation showing results
          
          ### Monitoring Alerts
          EOF
          
          if [ "$(echo "$HEALTH_DATA" | jq -r '.alert_needed')" = "true" ]; then
            echo "- ðŸš¨ **ALERT:** Health score below critical threshold" >> analysis/reports/monitoring-dashboard.md
          else
            echo "- âœ… **Status:** All systems operating within normal parameters" >> analysis/reports/monitoring-dashboard.md
          fi
          
          cat >> analysis/reports/monitoring-dashboard.md << EOF
          
          ## ðŸŽ¯ Next Actions
          
          ### Immediate (Next 24 hours)
          - Continue monitoring health improvements
          - Verify security workflow stability
          - Track performance optimizations
          
          ### Short-term (Next week)
          - Implement additional caching strategies
          - Optimize long-running workflows
          - Enhance error handling
          
          ### Long-term (Next month)
          - Advanced analytics implementation
          - Predictive health monitoring
          - Cross-repository benchmarking
          
          ---
          *Generated by Enhanced Monitoring Dashboard*
          EOF
          
          echo "âœ… Enhanced dashboard generated"

      - name: ðŸ“¤ Upload Monitoring Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-dashboard-${{ github.run_number }}
          path: |
            analysis/reports/
            analysis/metrics/
          retention-days: ${{ env.RETENTION_DAYS }}
          if-no-files-found: warn

  generate-alerts:
    name: ðŸš¨ Generate Health Alerts
    runs-on: ubuntu-latest
    needs: collect-metrics
    if: ${{ needs.collect-metrics.outputs.alert_needed == 'true' && github.event.inputs.enable_alerts != 'false' }}
    timeout-minutes: 10
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4

      - name: ðŸš¨ Create Health Alert Issue
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          HEALTH_SCORE: ${{ needs.collect-metrics.outputs.health_score }}
          HEALTH_STATUS: ${{ needs.collect-metrics.outputs.health_status }}
        run: |
          echo "ðŸš¨ Creating health alert issue..."
          
          gh issue create \
            --title "ðŸš¨ Workflow Health Alert: $HEALTH_STATUS (Score: $HEALTH_SCORE/100)" \
            --label "critical,monitoring,workflow-health" \
            --body "## ðŸ¥ Workflow Health Alert
          
          **Alert Level:** $HEALTH_STATUS
          **Health Score:** $HEALTH_SCORE/100
          **Detection Time:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          **Monitoring Run:** [View Details](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          
          ### ðŸ“Š Alert Details
          
          ðŸ”´ **CRITICAL:** Workflow health is severely degraded (Score: $HEALTH_SCORE/100)
          
          ### ðŸ” Investigation Steps
          
          1. Review Workflow Runs: Check recent workflow execution patterns
          2. Identify Failures: Look for patterns in failed workflows  
          3. Performance Analysis: Review execution times and queue delays
          4. Resource Check: Verify runner availability and performance
          
          ### ðŸ“‹ Immediate Actions Required
          
          - [ ] Review workflow failure patterns
          - [ ] Check for infrastructure issues
          - [ ] Optimize slow-running workflows
          - [ ] Update dependencies if needed
          - [ ] Monitor for continued degradation
          
          ### ðŸ“ˆ Reports Available
          
          â€¢ Comprehensive health report available in workflow artifacts
          â€¢ Performance analysis with timing details
          â€¢ Category-specific breakdown and recommendations
          
          --------
          
          This issue was automatically created by the Enhanced Monitoring Dashboard"

      - name: ðŸ“§ Send Alert Notification
        if: github.event.repository.private == false
        env:
          HEALTH_SCORE: ${{ needs.collect-metrics.outputs.health_score }}
          HEALTH_STATUS: ${{ needs.collect-metrics.outputs.health_status }}
        run: |
          echo "ðŸ“§ Alert notification sent"
          echo "Health Score: $HEALTH_SCORE"
          echo "Status: $HEALTH_STATUS"
          echo "Alert created successfully"

  summary:
    name: ðŸ“‹ Monitoring Summary
    runs-on: ubuntu-latest
    needs: [collect-metrics]
    if: always()
    timeout-minutes: 5
    
    steps:
      - name: ðŸ“Š Display Monitoring Summary
        env:
          HEALTH_SCORE: ${{ needs.collect-metrics.outputs.health_score }}
          HEALTH_STATUS: ${{ needs.collect-metrics.outputs.health_status }}
          TRENDING_DIRECTION: ${{ needs.collect-metrics.outputs.trending_direction }}
          MONITORING_SCOPE: ${{ github.event.inputs.monitoring_scope || 'comprehensive' }}
        run: |
          echo "ðŸ“Š Enhanced Monitoring Dashboard Summary"
          echo "========================================"
          echo "Health Score: $HEALTH_SCORE/100"
          echo "Status: $HEALTH_STATUS"
          echo "Trending: $TRENDING_DIRECTION"
          echo "Scope: $MONITORING_SCOPE"
          echo "Timestamp: $(date -u)"
          echo ""
          echo "ðŸŽ¯ Monitoring completed successfully!"
          echo "ðŸ“„ Check artifacts for detailed analysis"
